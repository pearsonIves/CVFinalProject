# CVFinalProject
Please note: this readme is a transcription of our project website. For a more complete experience and explanation, please see our project website - https://sites.google.com/vt.edu/siamesecelebrities/final-report

# Abstract
As humans, we take for granted our ability to recognize one another. While very simple to us, it is in reality a quite complex task for a machine. We sought to use modern methods of facial detection and image recognition to create a model that could learn to recognize the faces of people and use their faces to determine their identity. While not a perfect result, after many training sessions we were able to train a model with 76.19% accuracy. This is above our goal accuracy of 50%, and we believe is a strong result for a first time project in facial recognition and identification.

# Introduction
We hoped to use facial detection in combination with modern methods of machine learning to create a face recognition system. This functionality could be useful for a wide variety of reasons, including face identification, face authentication, image indexing, and more. We hoped that by combining facial detection and feature extraction functionality into one program, that we could create a more accurate program than one that did not detect faces before extracting a person's features. We feel that the value in our project is in the combination of existing ideas into one system, such as Haar Cascades for face detection, Convolutional Neural Networks for feature extraction, and a Siamese implementation of said CNNs for more accurate results in a limited dataset. 

# Approach
Our approach can be separated into three main parts, a general approach to detecting and identifying faces , and our specific approaches to training our CNN and Siamese CNN models.

## Identifying a face
In order to Identify a face, our program walks through four steps:
Facial Detection - In order to detect faces in an image, we decided to use HAAR Cascades and LBP cascades. These cascades are classifiers used for object detection. For our purposes, we used openCV's cascades to classify to create face classifiers and detect faces in our input images. This includes 5 HAAR cascades and 3LBP cascades to detect front and side-view faces. We would individually run each face classifier over the image to detect a face. If it did not detect a face, we would try another cascade until a face was found or we ran out of cascades. This method allowed us to only get one face from each image, while trying our best to ensure we got more than 0 faces. HAAR cascades were prioritized because they are generally more accurate.
Facial Extraction - After detecting a face, we would extract just the face from the entire image. We hoped this would improve our Siamese CNN's accuracy if it could train on just faces and not an individual's clothing and environment as well, because those can very easily change from image to image.
Image Normalization - Images need to be size normalized before they can be fed into a Neural-Network for training or evaluation. We resized all of our images to 128x128x3 as we felt this was a good standard size. We chose to keep color in our model as we felt skin and hair tones would be important information to keep available for our model.
Face Identification - Our normalized image would be fed into our most accurate CNN (after Siamese Training) in order to generate a feature vector for the image. We would then compare this to the feature vectors for all of our known identities (generated by the same CNN), and take the identity of the closest match and assign it as our guessed identity

## Approach to Building and Training our CNN
As our Siamese implementation would require us to use a CNN Model, we first needed to build the most accurate CNN model that we could. Both being beginners in machine learning, we decided to start with basic CNN models for image classification we could find tutorials for online. We found that Google's Tensorflow and Keras would be a good neural-network building tool for us to use, given it's beginner-friendliness and its integration in Google Colaboratory. "Colab" is a free service provided by Google that can host and execute Python code (among other languages). It also provides free access to Google's GPU's, which was a major advantage for us as training a Neural Network on a CPU is very slow, and neither of our laptops were equipped with GPUs.
Once we created a very simple model, we would then vary the structure and parameters of our CNN to test their effects on accuracy, and select the most model that resulted in the most accurate trained CNN. This selected model would then serve as the base model we would use and train in our Siamese implementation. Further details of this building and training can be found in the Experiments and Results section.

## Approach to Implementing and Training our Siamese CNN model
Due to our limited dataset size, we tried to approach the problem by implementing a Siamese CNN. This allowed us to generate a larger dataset than the one we started off with.  A Siamese CNN approach is one where the same CNN model takes in two images and gives us an n-dimensional feature vector that represents the images. Then we could use a number of ways to compare the vectors. We decided to use the Euclidian distance between the two vectors to compare them. Through passing this distance through a sigmoid function and getting an output between 0 and 1, we can train the CNN model using the label passed with pair of images. 
The pair of images are labeled 0 if they are of the same person and the pair are labeled 1 if they belong to two different people. As our training data now consists of pairs of images, we can generate a larger dataset due to the large number of per combinations of images. After  the training is complete, we can generate a feature vector for each person in our training set and store that in out database for facial recognition/matching. When we are then presented a new image, all we have to do is compute the feature vector of that image and measure its distance with the vectors in the database. Which ever vector in the database is closest to the feature vector of the new image is person the model predicts.
In our initial implementation of our CNN, we had three convolutional layers followed by a fully connected layer. Between each convolutional layer we had a Max Pooling layer to reduce the size of features, followed by a Dropout layer to prevent over fitting.The n-dimensional vector produced at the end of the CNN by the fully connected layer is our feature vector. Using 2 feature vectors produced by the CNN from a pair of images, we calculate the Euclidian distance between them and pass it through a sigmoid function to squeeze it between 0 and 1 to match our labels. Then, based on the difference between our prediction and the label, the CNN will learn better weights. It will try and produce feature vectors for images of the same person that are closer to each other.
We used the hinge loss function in our implementation as it was mentioned as a good loss function for training a CNN.

# Experiments and Results
For our purposes, we used the Five Faces dataset that can be found on Kaggle. We chose this data set because it's size of 118 images was much more manageable that many of the other identity-labeled face datasets available on the internet. This dataset also came pre divided into train/test parts. Other data sets ranged into the tens of thousands in terms of size, and we simply did not have the storage available on our local machines to download those data sets.
In order to determine how well our approaches were working, we measured the accuracy of our system. For our model, accuracy means correctly guessing/predicting the identity of the person in the image. Accuracy goes up when the model makes more correct identity assignments, and down when it makes more incorrect ones.
Some parameters/factors that we had to consider, and possibly vary, were the optimizer of our model, the learning rate, the batch size the data we passed, the dropout rate, and the kernel sizes of our model.
Optimizer - The algorithm used to minimize the loss of the neural network. 
Learning Rate - The learning rate of a neural network is sometimes considered to be the most important parameter. It determines the amount by which a Neural Network the weights of its nodes. If the learning rate is too low, a network can get stuck and never reach an optimal solution; Conversely, if the learning rate is too high, the model can converge on a solution that is non-optimal.
Batch size - For the CNN only implementation, we used the default Tensorflow settings. For the Siamese implementation, we used a batch size of 100 image pairs.
Dropout rate - We used a dropout rate of 0.2 to prevent overfitting.
Kernel size - We used different kernel sizes for different layers and different experiments. The kernel sizes used throughout this project are of size 7x7, 5x5 and 3x3.

## Siamese CNN experimentation and results

Learning rate = 0.0001
Resulting test accuracy = 58.3%

These were the results produced by the initial implementation of our Siamese CNN. The network seemed to plateau on training accuracy at 50%. Although the resulting test accuracy was above our pre determined criteria of 50%, it was not a consistent result.

Learning rate = 0.0001
Resulting test accuracy = 70.8%

In our second implementation, we changed all the kernels at each layer to be of size 3x3. The idea behind this decision was to better localize the convolutions and hence the feature maps which the network would train.
This implementation also did not seem to resolve the plateau in training accuracy, but could possibly generalize better.
Although it produced better results, it was still not consistent.



## CNN experimentation and results
Below are the results of training our CNN with different learning rates. There are the loss, accuracy, and validation accuracy values from the last epochs of the training as well as a graph plotting accuracy and validation (test) accuracy by epoch for each learning rate we tested. We found that while it took longer to train a network with our lowest learning rate of 0.0001, it resulted in a much higher accuracy than higher learning rates. In addition, learning rates that were higher than our highest learning rate of 0.001 would result in our model getting stuck in local minima (for example, getting steady accuracy values of 28%). In the end, our most successful model of all the ones that we had trained was not a Siamese implementation; In fact, it was just the facial extracted images being fed into a CNN for training with a learning rate of 0.001. After training this model for 150 epochs, it resulted in a test accuracy of 76.19%.
Once obtaining the parameters and model structure that would result in our best trained model, we decided to test our hypothesis that extracting a person's face prior to use in the model would result in a more accurate system. Using the same model parameters and structure (including input size, number and size of layers, activation type, learning rate, and more), but without extracting the face from the image, we trained a new model. This model reached a very steady test accuracy of 60% compared to the 76.19% we had previously achieved. We feel that this, to some degree, validates our hypothesis.

<result images can be found on our website>
  
# Conclusion and Future Work 
The report has presented two approaches for facial recognition. It starts off by taking a more complex approach that maybe more suitable for real world applications such as facial recognition for security purposes with a limited number of available images per person. 
Having not produced the best results with our Siamese implementation, we can see that falling back on a more traditional approach of just training a CNN to predict the class of a image works better, although it may require larger datasets and the need to be retrained for the addition of every new class/person.
To improve on our work, the trivial path would be to feed the network a larger dataset and more training cycles to improve on the accuracy. But this alone is not enough and could lead to overfitting. Therefore, another approach to be considered should be augmenting the training images to a) increase the size of the given dataset and b) allow the network to generalize better due to the addition of some "noise". In addition, we did see that pre-processing the input images (In our case this was face extraction) yielded more accurate results. It is possible that with further pre-processing operations we could further enhance our system.

# References
https://tianyusong.com/2017/11/16/pytorch-implementation%E2%80%8B-siamese-network/
https://machinelearningmastery.com/introduction-to-deep-learning-for-face-recognition/
https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html
https://pythonprogramming.net/haar-cascade-face-eye-detection-python-opencv-tutorial/
https://docs.opencv.org/3.4/dc/d88/tutorial_traincascade.html
https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html
https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3
https://research.google.com/colaboratory/faq.html
https://en.wikipedia.org/wiki/Convolutional_neural_network
https://www.kaggle.com/dansbecker/5-celebrity-faces-dataset
https://www.researchgate.net/figure/Gradient-Descent-Stuck-at-Local-Minima-18_fig4_338621083
